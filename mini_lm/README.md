# Project 02: Mini Language Model (Bigram)

This project implements a **Bigram Language Model** from scratch using Python and NumPy.

## What is a Bigram Language Model?

A Bigram Language Model is a type of **N-gram model** that predicts the probability of a word appearing in a sequence based only on the immediately preceding word. The term "bigram" refers to a sequence of two consecutive words.

The core idea is based on the **Markov Assumption**, which states that the future state (the next word) depends only on the current state (the previous word), and not on the entire history of the sequence.

Mathematically, the probability of a sequence of words $W = (w_1, w_2, \dots, w_n)$ is approximated as the product of the conditional probabilities of each word given the previous one:

$$P(W) \approx \prod_{i=2}^{n} P(w_i | w_{i-1})$$

The conditional probability $P(w_i | w_{i-1})$ is calculated from the training data using the frequency of the bigram $(w_{i-1}, w_i)$ divided by the frequency of the preceding word $w_{i-1}$:

$$P(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1}, w_i)}{\text{Count}(w_{i-1})}$$

## Project Structure and Implementation

| File           | Description                                                                                                                                            |
| :------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `bigram_lm.py` | Contains the `BigramLanguageModel` class, which handles tokenization, vocabulary building, training (counting and normalization), and text generation. |
| `main.py`      | The main execution script. It loads the corpus, trains the model, and demonstrates text generation by sampling from the learned probabilities.         |
| `corpus.txt`   | The sample training data (corpus) used to calculate the bigram probabilities.                                                                          |

### Key Implementation Details

1.  **Tokenization**: The input text is split into a list of words (tokens).
2.  **Vocabulary**: A unique list of all words is created, and each word is mapped to a numerical index.
3.  **Training**: A **transition matrix** (a NumPy array) is built. Each cell $(i, j)$ stores the probability of the word at index $j$ following the word at index $i$.
4.  **Smoothing**: **Laplace smoothing** (or Additive Smoothing) is applied during training to prevent zero probabilities for bigrams that did not appear in the training data. This is crucial for robust generation.
5.  **Generation**: New text is generated by starting with a seed word and iteratively selecting the next word based on the probability distribution in the transition matrix row corresponding to the current word.

## How to Run

1.  Ensure you have the `uv` package manager installed.
2.  Navigate to the project directory (`mini_lm`).
3.  Run the main script:

```bash
uv run main.py
```
